{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqAIBfwFBeST6BXRiT/N3N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exusia1/Cyberbullying_hw/blob/main/Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kZY53tytJDc"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import numpy as np\n",
        "import re\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7jiK7_9u-un"
      },
      "source": [
        "#files.upload()\n",
        "with open('/content/voyna-i-mir-tom-1.txt', 'r', encoding='windows-1251') as f:\n",
        "    text = f.read()\n",
        "    text = text.replace('\\ufeff', '') # убираем первый невидимый символ\n",
        "    text = re.sub(r'[^А-я ]', '', text) # заменяем все символы кроме кириллицы на пустые символы"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68sLmD_Utl25",
        "outputId": "f5a56a88-02a2-411f-bb4d-70d4c7c47710"
      },
      "source": [
        "# парсим текст, как последовательность символов\n",
        "num_characters = 34 # 33 буквы + пробел\n",
        "tokenizer = Tokenizer(num_words=num_characters, char_level=True) # токенизируем на уровне символов\n",
        "tokenizer.fit_on_texts([text]) # формируем токены на основе частотности внашем тексте\n",
        "print(tokenizer.word_index)\n",
        "inp_chars = 6\n",
        "data = tokenizer.texts_to_matrix(text) # преобразуем исходный текст в массив OHE\n",
        "n = data.shape[0] - inp_chars # так как мы предсказываем по трем символам - четвертый\n",
        "X = np.array([data[i:i + inp_chars, :] for i in range(n)])\n",
        "Y = data[inp_chars:] # предсказание следующего символа\n",
        "print(data.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'о': 2, 'а': 3, 'е': 4, 'и': 5, 'н': 6, 'т': 7, 'с': 8, 'л': 9, 'в': 10, 'р': 11, 'к': 12, 'д': 13, 'м': 14, 'у': 15, 'п': 16, 'я': 17, 'г': 18, 'ь': 19, 'ы': 20, 'з': 21, 'б': 22, 'ч': 23, 'й': 24, 'ж': 25, 'ш': 26, 'х': 27, 'ю': 28, 'ц': 29, 'э': 30, 'щ': 31, 'ф': 32, 'ъ': 33}\n",
            "(666713, 34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCNunVzBvGlq",
        "outputId": "abe87034-1acf-46a4-8c13-81481525a1b2"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input((inp_chars, num_characters))) \n",
        "# при тренировке в рекуррентные модели keras подается сразу вся последовательность, поэтому в input теперь два числа. 1-длина последовательности, 2-размер OHE\n",
        "model.add(SimpleRNN(128, activation='tanh')) # рекуррентный слой на 500 нейронов\n",
        "model.add(Dense(num_characters, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 128)               20864     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 34)                4386      \n",
            "=================================================================\n",
            "Total params: 25,250\n",
            "Trainable params: 25,250\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR0Qnr-p7nkt"
      },
      "source": [
        "history = model.fit(X, Y, batch_size=256, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUkiSfvRvJav"
      },
      "source": [
        "def buildPhrase(inp_str, str_len=50):\n",
        "    for i in range(str_len):\n",
        "        x = []\n",
        "        for j in range(i, i + inp_chars):\n",
        "            x.append(tokenizer.texts_to_matrix(inp_str[j])) # преобразуем символы в One-Hot-encoding\n",
        "        x = np.array(x)\n",
        "        inp = x.reshape(1, inp_chars, num_characters)\n",
        "        pred = model.predict(inp) # предсказываем OHE четвертого символа\n",
        "        d = tokenizer.index_word[pred.argmax(axis=1)[0]] # получаем ответв символьном представлении\n",
        "        inp_str += d # дописываем строку\n",
        "    return inp_str"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAM7q7FrvLTt",
        "outputId": "4596607c-67f5-4b6d-f29a-cc64b562f322"
      },
      "source": [
        "res = buildPhrase(\"утренн\")\n",
        "print(res)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "утренно сказал он                                       \n"
          ]
        }
      ]
    }
  ]
}